{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import hvplot.pandas\n",
    "import scipy.optimize\n",
    "import symbol_data\n",
    "import portfolio_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "spx_df = pd.read_csv(Path('./Resources/SPX.csv'), \n",
    "                    index_col='Date', \n",
    "                    parse_dates=True,\n",
    "                    infer_datetime_format=True\n",
    "                    )\n",
    "spx_daily_returns = spx_df['Close'].pct_change().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ['AAPL', 'ACN', 'ADBE', 'ADI', 'ADSK', 'AKAM', 'AMAT', 'AMD', 'CRM', 'DELL', 'DOCU', 'EPAM', 'FIS', 'FISV', 'IBM', 'INFY', 'LRCX', 'MSFT', 'MU', 'NOW', 'NVDA', 'ORCL', 'SAP', 'TEAM', 'TXN', 'WDAY', 'WORK', 'XLNX', 'ZBRA', 'ZEN', 'ZI', 'ZS']\n",
    "# This function may have to be rewritten to accomodate Anurag's data functions\n",
    "# as it performs all the analysis within the same loop as retrieving the data.\n",
    "def stock_data_calculator(stocks):\n",
    "    # Create an empty dictionary to store the stock data\n",
    "    stock_data_dict = {}\n",
    "    # This section can be replaced by Anurag's function\n",
    "    for stock in stocks:\n",
    "        # stock_df = pd.read_csv(\n",
    "        #     Path(f'./Resources/{stock}.csv'), \n",
    "        #     index_col = 'Date', \n",
    "        #     parse_dates = True, \n",
    "        #     infer_datetime_format=True\n",
    "        #     )\n",
    "        stock_df = portfolio_data.get_portfolio_historical_data(stock)\n",
    "        daily_returns = stock_df['Close'].pct_change().dropna()\n",
    "        daily_returns_df = pd.concat(\n",
    "            [daily_returns, spx_daily_returns], \n",
    "            axis=1,\n",
    "            join='inner', \n",
    "            keys = [stock, 'SPX']\n",
    "            )\n",
    "        covariance = daily_returns_df[stock].cov(daily_returns_df['SPX'])\n",
    "        beta = covariance / daily_returns_df['SPX'].var()\n",
    "\n",
    "        std = daily_returns.std()\n",
    "\n",
    "        var = daily_returns.var()\n",
    "\n",
    "        expected_return = .035 + beta*(.1-.035)\n",
    "        \n",
    "        stock_data_dict[stock] = {'expected_return': expected_return, 'std': std, 'var': var, 'beta': beta}\n",
    "    return stock_data_dict\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock_data_calculator(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_expected_return_calculator(stocks, weights):\n",
    "    # Create an empty list which will hold the expected returns\n",
    "    expected_returns = []\n",
    "    # Gather the stock data from the symbol_data_calculator function\n",
    "    symbol_data = stock_data_calculator(stocks)\n",
    "    # Pull only the expected returns from the data\n",
    "    for symbol in symbol_data:\n",
    "        expected_returns.append(symbol_data[symbol]['expected_return'])\n",
    "    # Multiply the expected returns by the weights to get weighted expected returns\n",
    "    weighted_expected_returns = expected_returns * weights\n",
    "    # Sum all these to get the expected return of the portfolio\n",
    "    weighted_expected_returns = weighted_expected_returns.sum()\n",
    "    return weighted_expected_returns \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = np.random.random(32)\n",
    "weights /= weights.sum()\n",
    "#portfolio_expected_return_calculator(stocks, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_variance_calculator(symbols, weights):\n",
    "    # Getting the data. This part can be replaced by anurag's function\n",
    "    df_dict = {}\n",
    "    for symbol in symbols:\n",
    "        symbol_df = pd.read_csv(\n",
    "            Path(f'./Resources/{symbol}.csv'),\n",
    "            index_col = 'Date',\n",
    "            parse_dates = True,\n",
    "            infer_datetime_format = True\n",
    "            )\n",
    "        df_dict[symbol] = symbol_df['Close']\n",
    "\n",
    "    prices_df = pd.concat(df_dict.values(), axis = 1, join = 'inner', keys = stocks)\n",
    "    daily_returns = prices_df.pct_change().dropna()\n",
    "    \n",
    "    # Get the covariance array of the portfolio using the .cov() method.\n",
    "    portfolio_cov = np.array(daily_returns.cov())\n",
    "\n",
    "    # Get the standard deviation of each asset in the portfolio using the .std() method.\n",
    "    portfolio_std = np.array(daily_returns.std())\n",
    "\n",
    "    # Cross multiply the std array with the transposition of itself.\n",
    "    portfolio_stdT = np.transpose(np.array([portfolio_std]))\n",
    "    portfolio_std_cp = portfolio_std * portfolio_stdT\n",
    "\n",
    "    # Get the correlation matrix by dividing the covariance matrix by the standard deviation matrix.\n",
    "    correlation_matrix = portfolio_cov / portfolio_std_cp\n",
    "\n",
    "    # Get weighted standard deviation and save the transposition of that array.\n",
    "    weighted_std = portfolio_std * weights\n",
    "    weighted_stdT = np.transpose(np.array([weighted_std]))\n",
    "\n",
    "    # Calculate portfolio variance by first multiplying the covariance matrix by the \n",
    "    # weighted standard deviation array. This will give an array which is \n",
    "    # 1x<the number of stocks in the portfolio>\n",
    "    portfolio_var = np.matmul(weighted_std, correlation_matrix)\n",
    "    # Then multiplying that array by the transposition of the weighted standard deviation array.\n",
    "    # This will give a single value.\n",
    "    portfolio_var = np.matmul(portfolio_var, weighted_stdT)\n",
    "    # And finally taking the square root of that value.\n",
    "    portfolio_var = np.sqrt(portfolio_var)\n",
    "    return portfolio_var\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01653808])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.random.random(32)\n",
    "weights /= weights.sum()\n",
    "portfolio_variance_calculator(stocks, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_performance_calculator(stocks, weights):\n",
    "    # This section can be replaced by Anurag's function\n",
    "    df_dict = {}\n",
    "    for stock in stocks:\n",
    "        stock_df = pd.read_csv(\n",
    "            Path(f'./Resources/{stock}.csv'),\n",
    "            index_col = 'Date',\n",
    "            parse_dates = True,\n",
    "            infer_datetime_format = True\n",
    "            )\n",
    "        stock_df['daily_returns'] = stock_df['Close'].pct_change().dropna()\n",
    "        df_dict[stock] = stock_df[['daily_returns']]\n",
    "\n",
    "    stocks_df = pd.concat(df_dict.values(), axis = 1, join = 'inner', keys = stocks)\n",
    "\n",
    "    # Calculate cumulative returns for each stock in the portfolio.\n",
    "    cumulative_returns = (1+stocks_df).cumprod() - 1\n",
    "    # Multiply each stock by its respective weight.\n",
    "    cumulative_returns = cumulative_returns * weights\n",
    "    # Sum all the weighted cumulative returns\n",
    "    cumulative_returns = cumulative_returns.sum(axis=1)\n",
    "    return cumulative_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function accepts a list of stocks and a list of weights which must be in the \n",
    "# same order. It will return the 95% confidence interval for annual performance\n",
    "# of the portfolio.\n",
    "def portfolio_95percent_confidence_calculator(stocks, weights):\n",
    "    expected_return = portfolio_expected_return_calculator(stocks, weights)\n",
    "    variance = portfolio_variance_calculator(stocks, weights)\n",
    "    lower_bound = expected_return - 2*variance\n",
    "    upper_bound = expected_return + 2*variance\n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['AAPL',\n",
       "  'ACN',\n",
       "  'ADBE',\n",
       "  'ADI',\n",
       "  'ADSK',\n",
       "  'AKAM',\n",
       "  'AMAT',\n",
       "  'AMD',\n",
       "  'CRM',\n",
       "  'DELL',\n",
       "  'DOCU',\n",
       "  'EPAM',\n",
       "  'FIS',\n",
       "  'FISV',\n",
       "  'IBM',\n",
       "  'INFY',\n",
       "  'LRCX',\n",
       "  'MSFT',\n",
       "  'MU',\n",
       "  'NOW',\n",
       "  'NVDA',\n",
       "  'ORCL',\n",
       "  'SAP',\n",
       "  'TEAM',\n",
       "  'TXN',\n",
       "  'WDAY',\n",
       "  'WORK',\n",
       "  'XLNX',\n",
       "  'ZBRA',\n",
       "  'ZEN',\n",
       "  'ZI',\n",
       "  'ZS'],\n",
       " array([0.02780538, 0.05813165, 0.00460112, 0.03942053, 0.02218883,\n",
       "        0.03479874, 0.04477446, 0.05167271, 0.05465561, 0.00801874,\n",
       "        0.05661067, 0.0304918 , 0.04938812, 0.02097647, 0.02898556,\n",
       "        0.05012794, 0.02899121, 0.05007483, 0.03715048, 0.01039015,\n",
       "        0.02792452, 0.03566118, 0.01763121, 0.04369109, 0.01798963,\n",
       "        0.05440701, 0.01613891, 0.02893466, 0.01389539, 0.00070229,\n",
       "        0.01564916, 0.01811997]))"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(stocks, weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def efficient_frontier_generator(stocks):\n",
    "    i = 0\n",
    "    portfolio_dict = {}\n",
    "    while i < 5:\n",
    "        weights = np.random.random(32)\n",
    "        weights /= weights.sum()\n",
    "        portfolio_return = portfolio_expected_return_calculator(stocks, weights)\n",
    "        portfolio_variance = portfolio_variance_calculator(stocks, weights)\n",
    "        portfolio_dict[i] = (portfolio_return, portfolio_variance, weights)\n",
    "        i += 1\n",
    "    return portfolio_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a naive approach to finding the portfolio with the minimum risk.\n",
    "# There is a way to minimize a function with SciPy.optimize.minimize but \n",
    "# I can't figure out how to do it. \n",
    "# This function takes a list of stocks and creates 100 random portfolios\n",
    "# and returns the one with the minimum risk.\n",
    "def naive_minimum_risk_finder(stocks):\n",
    "    # Create an iterator\n",
    "    i = 0\n",
    "    # Create dictionary to store the minimum risk portfolio\n",
    "    min_risk = {}\n",
    "    # Generate 100 portfolios with random weightings.\n",
    "    # Save only the one with the minimim risk.\n",
    "    while i < 100:\n",
    "        weights = np.random.random(len(stocks))\n",
    "        weights /= weights.sum()\n",
    "        risk = portfolio_variance_calculator(stocks, weights)\n",
    "        if not min_risk:\n",
    "            min_risk['min_risk']={'risk':risk, 'weights':weights}\n",
    "        elif risk < min_risk['min_risk']['risk']:\n",
    "            min_risk['min_risk']={'risk':risk, 'weights':weights}\n",
    "        i += 1\n",
    "    return min_risk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive_minimum_risk_finder(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes a naive approach to finding the portfolio with the minimum risk.\n",
    "# There is a way to minimize a function with SciPy.optimize.minimize but \n",
    "# I can't figure out how to do it. \n",
    "# This function takes a list of stocks and creates 100 random portfolios\n",
    "# and returns the one with the maximum return.\n",
    "def naive_maximum_return_finder(stocks):\n",
    "    i = 0\n",
    "    # Create dictionary to store the maximum return portfolio\n",
    "    max_return = {}\n",
    "    # Generate 100 portfolios with random weightings.\n",
    "    # Save only the one with the minimim risk.\n",
    "    while i < 100:\n",
    "        weights = np.random.random(len(stocks))\n",
    "        weights /= weights.sum()\n",
    "        returns = portfolio_expected_return_calculator(stocks, weights)\n",
    "        if not max_return:\n",
    "            max_return['max_return']={'return':returns, 'weights':weights}\n",
    "        elif returns > max_return['max_return']['return']:\n",
    "            max_return['max_return']={'return':returns, 'weights':weights}\n",
    "        i += 1\n",
    "    return max_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#naive_maximum_return_finder(stocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import db_initializer as di"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_data(symbol):\n",
    "    engine = di.return_engine_handler()\n",
    "    get_historical_data_query = f\"\"\"\n",
    "        SELECT * from SYMBOL_DETAILS_{symbol}\n",
    "    \"\"\"\n",
    "    stock_df =  pd.read_sql_query(get_historical_data_query,con=engine, index_col='Date')\n",
    "    stock_df.index = pd.to_datetime(stock_df.index, infer_datetime_format=True)\n",
    "    return stock_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1927-12-30</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1928-01-03</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>17.760000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1928-01-04</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>17.719999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1928-01-05</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>17.549999</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1928-01-06</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>17.660000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23318</th>\n",
       "      <td>2020-10-29</td>\n",
       "      <td>3277.169922</td>\n",
       "      <td>3341.050049</td>\n",
       "      <td>3259.820068</td>\n",
       "      <td>3310.110107</td>\n",
       "      <td>3310.110107</td>\n",
       "      <td>4903070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23319</th>\n",
       "      <td>2020-10-30</td>\n",
       "      <td>3293.590088</td>\n",
       "      <td>3304.929932</td>\n",
       "      <td>3233.939941</td>\n",
       "      <td>3269.959961</td>\n",
       "      <td>3269.959961</td>\n",
       "      <td>4840450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23320</th>\n",
       "      <td>2020-11-02</td>\n",
       "      <td>3296.199951</td>\n",
       "      <td>3330.139893</td>\n",
       "      <td>3279.739990</td>\n",
       "      <td>3310.239990</td>\n",
       "      <td>3310.239990</td>\n",
       "      <td>4310590000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23321</th>\n",
       "      <td>2020-11-03</td>\n",
       "      <td>3336.250000</td>\n",
       "      <td>3389.489990</td>\n",
       "      <td>3336.250000</td>\n",
       "      <td>3369.159912</td>\n",
       "      <td>3369.159912</td>\n",
       "      <td>4220070000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23322</th>\n",
       "      <td>2020-11-04</td>\n",
       "      <td>3406.459961</td>\n",
       "      <td>3486.250000</td>\n",
       "      <td>3405.169922</td>\n",
       "      <td>3443.439941</td>\n",
       "      <td>3443.439941</td>\n",
       "      <td>4783040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23323 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date         Open         High          Low        Close  \\\n",
       "0      1927-12-30    17.660000    17.660000    17.660000    17.660000   \n",
       "1      1928-01-03    17.760000    17.760000    17.760000    17.760000   \n",
       "2      1928-01-04    17.719999    17.719999    17.719999    17.719999   \n",
       "3      1928-01-05    17.549999    17.549999    17.549999    17.549999   \n",
       "4      1928-01-06    17.660000    17.660000    17.660000    17.660000   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "23318  2020-10-29  3277.169922  3341.050049  3259.820068  3310.110107   \n",
       "23319  2020-10-30  3293.590088  3304.929932  3233.939941  3269.959961   \n",
       "23320  2020-11-02  3296.199951  3330.139893  3279.739990  3310.239990   \n",
       "23321  2020-11-03  3336.250000  3389.489990  3336.250000  3369.159912   \n",
       "23322  2020-11-04  3406.459961  3486.250000  3405.169922  3443.439941   \n",
       "\n",
       "         Adj Close      Volume  \n",
       "0        17.660000           0  \n",
       "1        17.760000           0  \n",
       "2        17.719999           0  \n",
       "3        17.549999           0  \n",
       "4        17.660000           0  \n",
       "...            ...         ...  \n",
       "23318  3310.110107  4903070000  \n",
       "23319  3269.959961  4840450000  \n",
       "23320  3310.239990  4310590000  \n",
       "23321  3369.159912  4220070000  \n",
       "23322  3443.439941  4783040000  \n",
       "\n",
       "[23323 rows x 7 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_data.get_historical_data('SPX')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Project_01",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d97317d8a5f3767d631f8f5f9cb1ef3203497d7b6331512fb7f06b031b983eac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
